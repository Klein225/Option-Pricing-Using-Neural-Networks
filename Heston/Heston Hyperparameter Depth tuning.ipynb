{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Heston hyper-parameter Depth tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11910,
     "status": "ok",
     "timestamp": 1593680343580,
     "user": {
      "displayName": "Marc Klein",
      "photoUrl": "",
      "userId": "14297881284147348184"
     },
     "user_tz": -120
    },
    "id": "HhIih922RV9Q",
    "outputId": "ffcdf106-07a4-4271-8877-03f2db398536"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime, os\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from pylab import plt, mpl\n",
    "\n",
    "import time, timeit\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy import optimize\n",
    "import scipy.integrate as integrate\n",
    "import scipy.special as special \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmi8tz8lRV95"
   },
   "source": [
    "Load the Heston data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "oUV-M-yvRV97",
    "outputId": "29b2c37b-c99a-4bc2-84b1-112aabc043b4"
   },
   "outputs": [],
   "source": [
    "#To read the import the csv-file, use:\n",
    "raw_Options_input_train = pd.read_csv (r\"/Users/Marcklein/Desktop/Master Thesis/Option pricing using Neural Networks/Python/Heston/Heston_data_input_deep_train.csv\")\n",
    "raw_Options_output_train = pd.read_csv (r\"/Users/Marcklein/Desktop/Master Thesis/Option pricing using Neural Networks/Python/Heston/Heston_data_output_deep_train.csv\")\n",
    "raw_Options_input_test = pd.read_csv (r\"/Users/Marcklein/Desktop/Master Thesis/Option pricing using Neural Networks/Python/Heston/Heston_data_input_deep_test.csv\")\n",
    "raw_Options_output_test = pd.read_csv (r\"/Users/Marcklein/Desktop/Master Thesis/Option pricing using Neural Networks/Python/Heston/Heston_data_output_deep_test.csv\")\n",
    "\n",
    "#Creates some unnamed column in the beginning, delete it:\n",
    "del raw_Options_input_train['Unnamed: 0']\n",
    "del raw_Options_output_train['Unnamed: 0']\n",
    "del raw_Options_input_test['Unnamed: 0']\n",
    "del raw_Options_output_test['Unnamed: 0']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy it so we dont mess anything up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1593506558585,
     "user": {
      "displayName": "Marc Klein",
      "photoUrl": "",
      "userId": "14297881284147348184"
     },
     "user_tz": -120
    },
    "id": "UnMezLAORV9-",
    "outputId": "d2a7f1f7-b0b6-469b-dd9a-03da51121fb0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Options_input_train = raw_Options_input_train.copy()\n",
    "Options_output_train = raw_Options_output_train.copy()\n",
    "Options_input_test = raw_Options_input_test.copy()\n",
    "Options_output_test = raw_Options_output_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "altEsq-uRV-L"
   },
   "source": [
    "Since the standard deviation is calculated by taking the sum of the squared deviations from the mean, a zero standard deviation can only be possible when all the values of a variable are the same (all equal to the mean). In this case, those variables have no discriminative power so they can be removed from the analysis. They cannot improve any classification, clustering or regression task. Many implementations will do it for you or throw an error about a matrix calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zY0694xARV-O"
   },
   "source": [
    "### **Data preparation**\n",
    "\n",
    "We split our dataset into a training set and a test set (validation set is taken from the training set during model.fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-omBJ0xRV-O"
   },
   "outputs": [],
   "source": [
    "# 1/3 of total train-data-set for training and validating and 0.5 of total test-data-set for testting\n",
    "train_dataset = Options_input_train.sample(frac=0.3333333333333333, random_state=42)\n",
    "test_dataset = Options_input_test.sample(frac=0.5, random_state=42)\n",
    "\n",
    "train_labels = Options_output_train.sample(frac=0.3333333333333333, random_state=42)\n",
    "test_labels = Options_output_test.sample(frac=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yM9Vg1djRV-T"
   },
   "source": [
    "Check the overall statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1593506648378,
     "user": {
      "displayName": "Marc Klein",
      "photoUrl": "",
      "userId": "14297881284147348184"
     },
     "user_tz": -120
    },
    "id": "jcXaC5KDRV-U",
    "outputId": "38711e81-fb06-4cee-8b2d-f44b7713de02"
   },
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe().T\n",
    "test_stats = test_dataset.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1281,
     "status": "ok",
     "timestamp": 1593506654740,
     "user": {
      "displayName": "Marc Klein",
      "photoUrl": "",
      "userId": "14297881284147348184"
     },
     "user_tz": -120
    },
    "id": "7lk7tQZVRV-a",
    "outputId": "8a4b17d1-c7f7-4ce5-8deb-88422ff889c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input train data: (199651, 7)  Output train data: (199651, 10)\n",
      "Input test data: (9998, 7)  Output test data: (9998, 10)\n"
     ]
    }
   ],
   "source": [
    "#normalize the data\n",
    "def norm(x):\n",
    "    return (x - train_stats['min']) / (train_stats['max']-train_stats['min'])\n",
    "normed_train_data = norm(train_dataset).values\n",
    "\n",
    "def norm_test(x):\n",
    "    return (x - train_stats['min']) / (train_stats['max']-train_stats['min'])\n",
    "normed_test_data = norm_test(test_dataset).values\n",
    "\n",
    "#make the labels into numpy array just like the normed training data\n",
    "train_labels = np.asarray(train_labels)\n",
    "test_labels = np.asarray(test_labels)\n",
    "\n",
    "#check the shapes\n",
    "print(\"Input train data:\", normed_train_data.shape, \" Output train data:\", train_labels.shape)\n",
    "print(\"Input test data:\", normed_test_data.shape, \" Output test data:\", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8O9gU8FqRV-c"
   },
   "source": [
    "### **The hyperparameter testing-model**\n",
    "\n",
    "We start by initializing all the hyperparameters that we want to asses. We then set the metrics of the model to \"mean squared error\". Since Tensorboard works with log files that are created during the training process we create logs for the training process that records the losses, metrics and other measures during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The hyperparameters & their values to be tested are stored in a special type called HParam\n",
    "HP_LAYERS = hp.HParam('layers', hp.Discrete([2, 3, 4, 5, 6]))\n",
    "\n",
    "#Setting the Metric to MSE (Mean Squared Error)\n",
    "METRIC_MSE = 'mean_squared_error'\n",
    "\n",
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "#Creating & configuring log files\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_LAYERS],\n",
    "        metrics=[hp.Metric(METRIC_MSE, display_name='mean_squared_error')],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a function to train and validate the model which will take the hyperparameters as arguments. Each combination of hyperparameters will run for # epochs and the hyperparameters are provided in an hparams dictionary and used throughout the training function. In this notebook we will only focus on the depth parameter. There will therefore only be one hyper-parameter to tune!\n",
    "\n",
    "**DEPTH:**\n",
    "- https://math.stackexchange.com/questions/3335072/how-many-parameters-does-the-neural-network-have\n",
    "- https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889\n",
    "\n",
    "In Oosterlee et al. they say that for the Heston model, a DNN with 4 hidden layers and 400 neurons in each layer is the optimal choice. We will check the validity. In order to compare the depth effects we need the complexity to be approximately the same. The number of parameters (weights and biases) should therefore be as close as possible across the models. We will use their best guess as the one to go from. Recall inputs=7 and outputs=10, so we get that the # of parameters for the base model is 7*400+400*400+400*400+400*400+400*10=486,800 weights and 400+400+400+400+10=1,610 biases. In total we get #params=486,800+1,610=488,410"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate number of neurons for each layers (for one layer we will need 27133.333333 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from scipy import optimize\n",
    "\n",
    "def f(x):\n",
    "    #return((7*x+x*x+x*10) + (x+x+10) - 488410) # 2 layers\n",
    "    #return((7*x+x*x+x*x+x*10) + (x+x+x+10) - 488410) # 3 layers\n",
    "    return((7*x+x*x+x*x+x*x+x*10) + (x+x+x+x+10) - 488410) # 4 layers\n",
    "    #return((7*x+x*x+x*x+x*x+x*x+x*10) + (x+x+x+x+x+10) - 488410) # 5 layers\n",
    "    #return((7*x+x*x+x*x+x*x+x*x+x*x+x*10) + (x+x+x+x+x+x+10) - 488410) # 6 layers\n",
    "\n",
    "root = optimize.brentq(f, 0, 1000)\n",
    "root''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight and bias initializers\n",
    "weights_initializer = keras.initializers.GlorotUniform(seed=42)\n",
    "bias_initializer = keras.initializers.Zeros()\n",
    "\n",
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "\n",
    "#A function that trains and validates the model on a variety of hyper-parameters and returns the MSE\n",
    "def train_val_model(hparams):\n",
    "    #Keras sequential model with Hyperparameters passed from the argument\n",
    "    if hparams[HP_LAYERS] == 2:\n",
    "         model = keras.models.Sequential([\n",
    "             #Layer to be used as an entry point into a Network (true number of neurons 689.4207751955869)\n",
    "             keras.layers.InputLayer(input_shape=[len(train_dataset.keys())]),\n",
    "             #Dense layer 1   \n",
    "             keras.layers.Dense(689, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_1'),\n",
    "             #Dense layer 2\n",
    "             keras.layers.Dense(689, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_2'),\n",
    "             #Output layer\n",
    "             keras.layers.Dense(10, activation='linear', name='Output_layer')])\n",
    "            \n",
    "            \n",
    "    elif hparams[HP_LAYERS] == 3:\n",
    "        model = keras.models.Sequential([\n",
    "             #Layer to be used as an entry point into a Network (true number of neurons 489.1912585224469)\n",
    "             keras.layers.InputLayer(input_shape=[len(train_dataset.keys())]),                \n",
    "             #Dense layer 1   \n",
    "             keras.layers.Dense(489, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_1'),\n",
    "             #Dense layer 2\n",
    "             keras.layers.Dense(489, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_2'),\n",
    "            #Dense layer 3\n",
    "             keras.layers.Dense(489, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_3'),\n",
    "             #Output layer\n",
    "             keras.layers.Dense(10, activation='linear', name='Output_layer')])\n",
    "        \n",
    "        \n",
    "    elif hparams[HP_LAYERS] == 4:\n",
    "        model = keras.models.Sequential([\n",
    "             #Layer to be used as an entry point into a Network (base number of neurons)\n",
    "             keras.layers.InputLayer(input_shape=[len(train_dataset.keys())]), \n",
    "             #Dense layer 1   \n",
    "             keras.layers.Dense(400, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_1'),\n",
    "             #Dense layer 2\n",
    "             keras.layers.Dense(400, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_2'),\n",
    "            #Dense layer 3\n",
    "             keras.layers.Dense(400, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_3'),\n",
    "            #Dense layer 4\n",
    "             keras.layers.Dense(400, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_4'),\n",
    "             #Output layer\n",
    "             keras.layers.Dense(10, activation='linear', name='Output_layer')])\n",
    "        \n",
    "        \n",
    "    elif hparams[HP_LAYERS] == 5:\n",
    "        model = keras.models.Sequential([\n",
    "             #Layer to be used as an entry point into a Network (true number of neurons 346.68892527879603)\n",
    "             keras.layers.InputLayer(input_shape=[len(train_dataset.keys())]), \n",
    "             #Dense layer 1   \n",
    "             keras.layers.Dense(347, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_1'),\n",
    "             #Dense layer 2\n",
    "             keras.layers.Dense(347, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_2'),\n",
    "            #Dense layer 3\n",
    "             keras.layers.Dense(347, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_3'),\n",
    "            #Dense layer 4\n",
    "             keras.layers.Dense(347, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_4'),\n",
    "            #Dense layer 5\n",
    "             keras.layers.Dense(347, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_5'),\n",
    "             #Output layer\n",
    "             keras.layers.Dense(10, activation='linear', name='Output_layer')])\n",
    "        \n",
    "        \n",
    "    elif hparams[HP_LAYERS] == 6:\n",
    "        model = keras.models.Sequential([\n",
    "             #Layer to be used as an entry point into a Network (true number of neurons 310.2464605462682)\n",
    "             keras.layers.InputLayer(input_shape=[len(train_dataset.keys())]), \n",
    "             #Dense layer 1   \n",
    "             keras.layers.Dense(310, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_1'),\n",
    "             #Dense layer 2\n",
    "             keras.layers.Dense(310, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_2'),\n",
    "            #Dense layer 3\n",
    "             keras.layers.Dense(310, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_3'),\n",
    "            #Dense layer 4\n",
    "             keras.layers.Dense(310, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_4'),\n",
    "            #Dense layer 5\n",
    "             keras.layers.Dense(310, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_5'),\n",
    "            #Dense layer 6\n",
    "             keras.layers.Dense(310, kernel_initializer = weights_initializer,\n",
    "                                activation = 'relu', bias_initializer = bias_initializer, name='Layer_6'),\n",
    "             #Output layer\n",
    "             keras.layers.Dense(10, activation='linear', name='Output_layer')])\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"unexpected layer number: %r\" % (hparams[HP_LAYERS],))\n",
    "        \n",
    "            \n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999,\n",
    "                             epsilon=1e-07, amsgrad=False, name='Adam')\n",
    "    #Compiling the model\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='mean_squared_error', #Computes the mean of squares of errors between labels and predictions\n",
    "                  metrics=['mean_squared_error']) #Computes the mean squared error between y_true and y_pred\n",
    "    \n",
    "    #Training the network\n",
    "    model.fit(normed_train_data, train_labels, \n",
    "         batch_size=32, \n",
    "         epochs=50,\n",
    "         verbose=0,\n",
    "         validation_split=0.2,\n",
    "         callbacks=[PrintDot()])\n",
    "    \n",
    "    _, mse = model.evaluate(normed_test_data, test_labels)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will initiate the training process with the hyperparameters to be assessed and will create a summary based on the MSE value returned by the train_test_model function and writes the summary with the hyperparameters and final accuracy(MSE) in logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(run_dir, hparams):\n",
    "    with tf.summary.create_file_writer(run_dir).as_default():\n",
    "        hp.hparams(hparams)  # record the values used in this trial\n",
    "        mse = train_val_model(hparams)\n",
    "        tf.summary.scalar(METRIC_MSE, mse, step=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now train the model for each combination of the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting trial: run-0\n",
      "{'layers': 2}\n",
      "\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 4.2742e-07 - mean_squared_error: 4.2742e-07\n",
      "Time used for trial: run-0, took 1155.29 seconds\n",
      "\n",
      "--- Starting trial: run-1\n",
      "{'layers': 3}\n",
      "\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 5.3981e-07 - mean_squared_error: 5.3981e-07\n",
      "Time used for trial: run-1, took 1210.04 seconds\n",
      "\n",
      "--- Starting trial: run-2\n",
      "{'layers': 4}\n",
      "\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 6.0061e-07 - mean_squared_error: 6.0061e-07\n",
      "Time used for trial: run-2, took 975.83 seconds\n",
      "\n",
      "--- Starting trial: run-3\n",
      "{'layers': 5}\n",
      "\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 6.0738e-07 - mean_squared_error: 6.0738e-07\n",
      "Time used for trial: run-3, took 1133.82 seconds\n",
      "\n",
      "--- Starting trial: run-4\n",
      "{'layers': 6}\n",
      "\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 1.0331e-06 - mean_squared_error: 1.0331e-06\n",
      "Time used for trial: run-4, took 1036.56 seconds\n",
      "\n",
      "CPU times: user 3h 55min 29s, sys: 16min 59s, total: 4h 12min 28s\n",
      "Wall time: 1h 31min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#A unique number for each training session\n",
    "session_num = 0\n",
    "\n",
    "#Nested for loop training with all possible  combinathon of hyperparameters\n",
    "for layers in HP_LAYERS.domain.values:\n",
    "    hparams = {\n",
    "        HP_LAYERS: layers\n",
    "        }\n",
    "    run_name = \"run-%d\" % session_num\n",
    "    print('--- Starting trial: %s' % run_name)\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "    start = timeit.default_timer()\n",
    "    run('logs/hparam_tuning/' + run_name, hparams)\n",
    "    elapsed_time = timeit.default_timer() - start\n",
    "    print('Time used for trial: {}, took {:.2f} seconds\\n'.format(run_name, elapsed_time))\n",
    "    session_num += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Itâ€™s time to launch TensorBoard. Use the following commands to launch tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1992), started 14:32:42 ago. (Use '!kill 1992' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-82c0e5b0e231c80f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-82c0e5b0e231c80f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/hparam_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is launched, you will see a beautiful dashboard. Click on the HPARAMS tab to see the hyperparameter logs.\n",
    "\n",
    "In \"Table View\" all the hyperparameter combinations and the respective accuracy will be displayed in a beautiful table as. The left side of the dashboard provides a number of filtering capabilities such as sorting based on the metric, filtering based on specific type or value of hyperparameter, filtering based on status etc.\n",
    "\n",
    "The Parallel Coordinates View shows each run as a line going through an axis for each hyperparameter and metric. The interactive plot allows us to mark a region which will highlight only the runs that pass through it. The units if each hyperparameter can also be changed between linear, logarithmic and quantile values. This is extremely useful in understanding the relationships between the hyperparameters. We can select the optimum hyperparameters just by selecting the least MSE (run your mouse over the line)\n",
    "\n",
    "The Scatter Plot View plots each of the hyperparameter and the given metric against the metric.This helps us understand how different values of each parameter correlates to the metric.\n",
    "\n",
    "LINKS:\n",
    "\n",
    "https://analyticsindiamag.com/parameter-tuning-tensorboard/\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
    "\n",
    "https://medium.com/ml-book/neural-networks-hyperparameter-tuning-in-tensorflow-2-0-a7b4e2b574a1\n",
    "\n",
    "https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/hparams/summary_v2.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAS: \n",
    "\n",
    "- HP_LEARNING_RATE = hp.HParam(\"learning_rate\", hp.RealInterval(1e-5, 1e-1))\n",
    "\n",
    "- HP_L2 = hp.HParam('l2 regularizer', hp.RealInterval(.001,.01))\n",
    "\n",
    "- HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.3, 0.8))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "The Heston Model-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
